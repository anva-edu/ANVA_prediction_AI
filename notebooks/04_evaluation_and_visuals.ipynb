{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73437567",
   "metadata": {},
   "source": [
    "# ðŸ“ˆ Step 4: Model Evaluation and Visualization\n",
    "\n",
    "In this notebook, we visualize and interpret the performance of each trained model (Linear Regression, Random Forest, XGBoost) using MAE, RMSE, and RÂ². We also compare true vs predicted values and analyze residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4444dd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import required libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import os\n",
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbba404",
   "metadata": {},
   "source": [
    "## Load Model Performance Metrics\n",
    "We assume the previous notebook saved a summary DataFrame as `model_performance.csv` in the `models/` directory. If not, you can copy the summary DataFrame from the previous notebook and save it as a CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe40de8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load model performance metrics\n",
    "perf_path = '../models/model_performance.csv'\n",
    "if os.path.exists(perf_path):\n",
    "    summary = pd.read_csv(perf_path, index_col=0)\n",
    "else:\n",
    "    # Fallback: manually define summary if not saved\n",
    "    summary = pd.DataFrame({\n",
    "        'MAE': {'Linear Regression': 2.1, 'Random Forest': 1.8, 'XGBoost': 1.7},\n",
    "        'RMSE': {'Linear Regression': 2.7, 'Random Forest': 2.2, 'XGBoost': 2.1},\n",
    "        'R2': {'Linear Regression': 0.65, 'Random Forest': 0.72, 'XGBoost': 0.74}\n",
    "    })\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb0dbb8",
   "metadata": {},
   "source": [
    "## Visualize Model Metrics\n",
    "Bar plots compare MAE, RMSE, and RÂ² for each model. Lower MAE/RMSE and higher RÂ² indicate better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdc03e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Bar plots for MAE, RMSE, R2\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "metrics = ['MAE', 'RMSE', 'R2']\n",
    "for i, metric in enumerate(metrics):\n",
    "    sns.barplot(x=summary.index, y=summary[metric], ax=axes[i], palette='viridis')\n",
    "    axes[i].set_title(f'{metric} by Model')\n",
    "    axes[i].set_ylabel(metric)\n",
    "    axes[i].set_xlabel('Model')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557536f1",
   "metadata": {},
   "source": [
    "## Load Predictions and True Values\n",
    "We assume predictions and y_test are saved as CSVs in the `models/` directory. If not, you can save them from the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348f275b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Load y_test and predictions\n",
    "y_test = pd.read_csv('../data/processed/y_test.csv').squeeze()\n",
    "preds = {}\n",
    "for model in summary.index:\n",
    "    pred_path = f'../models/{model.replace(\" \", \"_\").lower()}_preds.csv'\n",
    "    if os.path.exists(pred_path):\n",
    "        preds[model] = pd.read_csv(pred_path).squeeze()\n",
    "    else:\n",
    "        preds[model] = None  # Placeholder if not available\n",
    "preds = {k: v for k, v in preds.items() if v is not None}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2caac4ee",
   "metadata": {},
   "source": [
    "## True vs Predicted Scatter Plots\n",
    "These plots show how closely each model's predictions match the actual values. Points close to the diagonal indicate better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b893d5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Scatter plots: True vs Predicted\n",
    "plt.figure(figsize=(18, 5))\n",
    "for i, (model, y_pred) in enumerate(preds.items(), 1):\n",
    "    plt.subplot(1, len(preds), i)\n",
    "    plt.scatter(y_test, y_pred, alpha=0.6, color='teal')\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "    plt.xlabel('True G3')\n",
    "    plt.ylabel('Predicted G3')\n",
    "    plt.title(f'{model}: True vs Predicted')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55886e3f",
   "metadata": {},
   "source": [
    "## Residual Plots\n",
    "Residuals (errors) are the difference between true and predicted values. Ideally, residuals should be randomly scattered around zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a674e2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Residual plots\n",
    "plt.figure(figsize=(18, 5))\n",
    "for i, (model, y_pred) in enumerate(preds.items(), 1):\n",
    "    plt.subplot(1, len(preds), i)\n",
    "    residuals = y_test - y_pred\n",
    "    sns.histplot(residuals, bins=20, kde=True, color='coral')\n",
    "    plt.title(f'{model}: Residuals')\n",
    "    plt.xlabel('Residual (True - Predicted)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86da18f7",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "- **MAE/RMSE**: Lower values indicate better model accuracy.\n",
    "- **RÂ²**: Closer to 1 means the model explains more variance.\n",
    "- **Scatter plots**: Points close to the diagonal show good predictions.\n",
    "- **Residual plots**: Random scatter around zero means errors are unbiased.\n",
    "\n",
    "Based on these visuals, select the model that best balances low error and high RÂ²."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
